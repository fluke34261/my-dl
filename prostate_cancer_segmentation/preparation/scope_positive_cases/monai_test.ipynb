{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "802ee439-4603-4914-9edc-5517006d5ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: monai in /home/mpmri/miniconda3/envs/nnv2/lib/python3.11/site-packages (1.2.0)\n",
      "Requirement already satisfied: torch>=1.9 in /home/mpmri/miniconda3/envs/nnv2/lib/python3.11/site-packages (from monai) (2.0.1)\n",
      "Requirement already satisfied: numpy>=1.20 in /home/mpmri/miniconda3/envs/nnv2/lib/python3.11/site-packages (from monai) (1.25.0)\n",
      "Requirement already satisfied: filelock in /home/mpmri/miniconda3/envs/nnv2/lib/python3.11/site-packages (from torch>=1.9->monai) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions in /home/mpmri/miniconda3/envs/nnv2/lib/python3.11/site-packages (from torch>=1.9->monai) (4.6.3)\n",
      "Requirement already satisfied: sympy in /home/mpmri/miniconda3/envs/nnv2/lib/python3.11/site-packages (from torch>=1.9->monai) (1.12)\n",
      "Requirement already satisfied: networkx in /home/mpmri/miniconda3/envs/nnv2/lib/python3.11/site-packages (from torch>=1.9->monai) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/mpmri/miniconda3/envs/nnv2/lib/python3.11/site-packages (from torch>=1.9->monai) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/mpmri/miniconda3/envs/nnv2/lib/python3.11/site-packages (from torch>=1.9->monai) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/mpmri/miniconda3/envs/nnv2/lib/python3.11/site-packages (from torch>=1.9->monai) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/mpmri/miniconda3/envs/nnv2/lib/python3.11/site-packages (from torch>=1.9->monai) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/mpmri/miniconda3/envs/nnv2/lib/python3.11/site-packages (from torch>=1.9->monai) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/mpmri/miniconda3/envs/nnv2/lib/python3.11/site-packages (from torch>=1.9->monai) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/mpmri/miniconda3/envs/nnv2/lib/python3.11/site-packages (from torch>=1.9->monai) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/mpmri/miniconda3/envs/nnv2/lib/python3.11/site-packages (from torch>=1.9->monai) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/mpmri/miniconda3/envs/nnv2/lib/python3.11/site-packages (from torch>=1.9->monai) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/mpmri/miniconda3/envs/nnv2/lib/python3.11/site-packages (from torch>=1.9->monai) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/mpmri/miniconda3/envs/nnv2/lib/python3.11/site-packages (from torch>=1.9->monai) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/mpmri/miniconda3/envs/nnv2/lib/python3.11/site-packages (from torch>=1.9->monai) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/mpmri/miniconda3/envs/nnv2/lib/python3.11/site-packages (from torch>=1.9->monai) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /home/mpmri/miniconda3/envs/nnv2/lib/python3.11/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.9->monai) (67.8.0)\n",
      "Requirement already satisfied: wheel in /home/mpmri/miniconda3/envs/nnv2/lib/python3.11/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.9->monai) (0.38.4)\n",
      "Requirement already satisfied: cmake in /home/mpmri/miniconda3/envs/nnv2/lib/python3.11/site-packages (from triton==2.0.0->torch>=1.9->monai) (3.26.4)\n",
      "Requirement already satisfied: lit in /home/mpmri/miniconda3/envs/nnv2/lib/python3.11/site-packages (from triton==2.0.0->torch>=1.9->monai) (16.0.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/mpmri/miniconda3/envs/nnv2/lib/python3.11/site-packages (from jinja2->torch>=1.9->monai) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/mpmri/miniconda3/envs/nnv2/lib/python3.11/site-packages (from sympy->torch>=1.9->monai) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install monai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9dea607",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from medpy.io import load, save\n",
    "\n",
    "import monai\n",
    "from monai.config import print_config\n",
    "from monai.data import DataLoader, Dataset\n",
    "from monai.networks.nets import SegResNet\n",
    "from monai.losses import DiceLoss\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.transforms import (\n",
    "    Activations,\n",
    "    Activationsd,\n",
    "    AsDiscrete,\n",
    "    AsDiscreted,\n",
    "    LoadImage,\n",
    "    LoadImaged,\n",
    "    AddChanneld,\n",
    "    EnsureChannelFirstd,\n",
    "    AsChannelFirstd,\n",
    "    AsChannelLastd,\n",
    "    Compose,\n",
    "    RandRotate90d,\n",
    "    Resized,\n",
    "    ScaleIntensityd,\n",
    "    ConcatItemsd,\n",
    "    ToTensord\n",
    ")\n",
    "\n",
    "pin_memory = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "74a432d2",
   "metadata": {},
   "source": [
    "https://github.com/Project-MONAI/tutorials/blob/main/3d_segmentation/brats_segmentation_3d.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4593d2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 0\n",
    "path = '/media/mpmri/mpmri/fluke/workdir/nnUNet_raw_data/'\n",
    "dataset = f\"{path}Task2213_picai_fluke\"\n",
    "dir_image = f\"{dataset}/imagesTr/\"\n",
    "dir_label = f\"{dataset}/labelsTr/\"\n",
    "dataset_js = json.load(open(f\"{dataset}/dataset.json\"))\n",
    "split_js = json.load(open(f\"{dataset}/splits.json\"))\n",
    "\n",
    "batch_size = 2\n",
    "image_size = (256,256,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "508a441a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list(img_list):\n",
    "    file_list = []\n",
    "    for img in img_list:\n",
    "        one = {'t2w':f\"{dir_image}{img}_0000.nii.gz\",\n",
    "               'adc':f\"{dir_image}{img}_0001.nii.gz\",\n",
    "               'hbv':f\"{dir_image}{img}_0002.nii.gz\",\n",
    "               'label':f\"{dir_label}{img}.nii.gz\"}\n",
    "        file_list.append(one)\n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00d80ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path_nii):\n",
    "    image_data, image_header = load(path_nii)\n",
    "    image_sitk = image_header.get_sitkimage()\n",
    "    image_direction = image_header.get_direction()\n",
    "    image_offset = image_header.get_offset()\n",
    "    image_voxel_spacing = image_header.get_voxel_spacing()\n",
    "    return image_data, image_sitk, image_header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c1e9118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_file(path):\n",
    "    image_data, image_sitk, image_header = load_image(path)\n",
    "    \n",
    "    # print(image_sitk)\n",
    "    print(image_data.shape)\n",
    "    \n",
    "    plt.figure(figsize=(16,16))\n",
    "    for i in range(image_data.shape[2]):\n",
    "        plt.subplot(6,6,i+1)\n",
    "        plt.imshow(image_data[:,:,i], cmap='gray')\n",
    "        plt.tick_params(axis='both',\n",
    "                        which='both',\n",
    "                        bottom=False,\n",
    "                        left=False,\n",
    "                        top=False,\n",
    "                        labelbottom=False,\n",
    "                        labelleft=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87fb9abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_split = split_js[fold]\n",
    "train_list = get_list(current_split['train'])\n",
    "valid_list = get_list(current_split['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "005fc985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure image transform\n",
    "train_transforms = Compose([\n",
    "    LoadImaged(keys=[\"t2w\", \"adc\", \"hbv\", \"label\"], image_only=True),\n",
    "    ScaleIntensityd(keys=[\"t2w\", \"adc\", \"hbv\"]),\n",
    "    EnsureChannelFirstd(keys=[\"t2w\", \"adc\", \"hbv\", \"label\"]),\n",
    "    Resized(keys=[\"t2w\", \"adc\", \"hbv\", \"label\"], spatial_size=image_size),\n",
    "    ConcatItemsd(keys=[\"t2w\", \"adc\", \"hbv\"], name=\"input\"),\n",
    "    RandRotate90d(keys=[\"input\", \"label\"]),\n",
    "    ToTensord(keys=[\"input\", \"label\"])\n",
    "])\n",
    "test_transforms = Compose([\n",
    "    LoadImaged(keys=[\"t2w\", \"adc\", \"hbv\", \"label\"], image_only=True),\n",
    "    ScaleIntensityd(keys=[\"t2w\", \"adc\", \"hbv\"]),\n",
    "    EnsureChannelFirstd(keys=[\"t2w\", \"adc\", \"hbv\", \"label\"]),\n",
    "    Resized(keys=[\"t2w\", \"adc\", \"hbv\", \"label\"], spatial_size=image_size),\n",
    "    ConcatItemsd(keys=[\"t2w\", \"adc\", \"hbv\"], name=\"input\"),\n",
    "    ToTensord(keys=[\"input\", \"label\"])\n",
    "])\n",
    "check_ds = Dataset(data=train_list, transform=train_transforms)\n",
    "check_loader = DataLoader(check_ds, \n",
    "                          batch_size=4, \n",
    "                          num_workers=4, \n",
    "                          pin_memory=pin_memory)\n",
    "\n",
    "one_batch = next(iter(check_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78db23d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([4, 3, 256, 256, 32])\n",
      "Labels batch shape:  torch.Size([4, 1, 256, 256, 32])\n",
      "<class 'monai.data.meta_tensor.MetaTensor'> torch.Size([4, 3, 256, 256, 32]) torch.Size([4, 1, 256, 256, 32])\n"
     ]
    }
   ],
   "source": [
    "# debug data loader\n",
    "print(f\"Feature batch shape: {one_batch['input'].size()}\")\n",
    "print(f\"Labels batch shape:  {one_batch['label'].size()}\")\n",
    "print(type(one_batch['input']), one_batch['input'].shape, one_batch['label'].shape)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "201a6a88",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(16,16))\n",
    "plt.title('T2W')\n",
    "for ei, i in enumerate(range(0,one_batch['input'].size()[-1],2)):\n",
    "    plt.subplot(6,6,ei+1)\n",
    "    plt.imshow(one_batch['input'][0][0][:,:,i], cmap='gray')\n",
    "    \n",
    "plt.figure(figsize=(16,16))\n",
    "plt.title('ADC')\n",
    "for ei, i in enumerate(range(0,one_batch['input'].size()[-1],2)):\n",
    "    plt.subplot(6,6,ei+1)\n",
    "    plt.imshow(one_batch['input'][0][1][:,:,i], cmap='gray')\n",
    "    \n",
    "plt.figure(figsize=(16,16))\n",
    "plt.title('HBV')\n",
    "for ei, i in enumerate(range(0,one_batch['input'].size()[-1],2)):\n",
    "    plt.subplot(6,6,ei+1)\n",
    "    plt.imshow(one_batch['input'][0][2][:,:,i], cmap='gray')\n",
    "    \n",
    "plt.figure(figsize=(16,16))\n",
    "plt.title('Label')\n",
    "for ei, i in enumerate(range(0,one_batch['label'].size()[-1],2)):\n",
    "    plt.subplot(6,6,ei+1)\n",
    "    plt.imshow(one_batch['label'][0][0][:,:,i], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f36ce91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a training data loader (balanced)\n",
    "train_ds = Dataset(data=train_list, transform=train_transforms)\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, num_workers=4, pin_memory=pin_memory, shuffle=True)\n",
    "\n",
    "# create a validation data loader\n",
    "val_ds = Dataset(data=valid_list, transform=test_transforms)\n",
    "val_loader = DataLoader(val_ds, batch_size=1, num_workers=1, pin_memory=pin_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1ff7b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([2, 3, 256, 256, 32])\n",
      "Labels batch shape: torch.Size([2, 1, 256, 256, 32])\n"
     ]
    }
   ],
   "source": [
    "# debug data loader \n",
    "sample_batch = next(iter(train_loader))\n",
    "print(f\"Feature batch shape: {sample_batch['input'].size()}\")\n",
    "print(f\"Labels batch shape: {sample_batch['label'].size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "692cdabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 300\n",
    "val_interval = 1\n",
    "VAL_AMP = True\n",
    "\n",
    "# standard PyTorch program style: create SegResNet, DiceLoss and Adam optimizer\n",
    "device = torch.device(\"cuda\")\n",
    "model = SegResNet(\n",
    "    blocks_down=[1, 2, 2, 4],\n",
    "    blocks_up=[1, 1, 1],\n",
    "    init_filters=16,\n",
    "    in_channels=3,\n",
    "    out_channels=3,\n",
    "    dropout_prob=0.2,\n",
    ").to(device)\n",
    "loss_function = DiceLoss(smooth_nr=0, smooth_dr=1e-5, squared_pred=True, to_onehot_y=False, sigmoid=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), 1e-4, weight_decay=1e-5)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_epochs)\n",
    "\n",
    "dice_metric = DiceMetric(include_background=True, reduction=\"mean\")\n",
    "dice_metric_batch = DiceMetric(include_background=True, reduction=\"mean_batch\")\n",
    "\n",
    "post_trans = Compose([Activations(sigmoid=True), AsDiscrete(threshold=0.5)])\n",
    "\n",
    "\n",
    "# define inference method\n",
    "def inference(input):\n",
    "    def _compute(input):\n",
    "        return sliding_window_inference(\n",
    "            inputs=input,\n",
    "            roi_size=(240, 240, 160),\n",
    "            sw_batch_size=1,\n",
    "            predictor=model,\n",
    "            overlap=0.5,\n",
    "        )\n",
    "\n",
    "    if VAL_AMP:\n",
    "        with torch.cuda.amp.autocast():\n",
    "            return _compute(input)\n",
    "    else:\n",
    "        return _compute(input)\n",
    "\n",
    "# use amp to accelerate training\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "# enable cuDNN benchmark\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5108273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 1/300\n",
      "1/517, train_loss: 0.7140, step time: 5.7146\n",
      "2/517, train_loss: 0.6836, step time: 0.2788\n",
      "3/517, train_loss: 0.6772, step time: 0.2672\n",
      "4/517, train_loss: 0.6681, step time: 0.2685\n",
      "5/517, train_loss: 0.6647, step time: 0.2749\n",
      "6/517, train_loss: 0.6522, step time: 0.2698\n",
      "7/517, train_loss: 0.6633, step time: 0.2694\n",
      "8/517, train_loss: 0.6334, step time: 0.2739\n",
      "9/517, train_loss: 0.6662, step time: 0.2736\n",
      "10/517, train_loss: 0.6537, step time: 0.2721\n",
      "11/517, train_loss: 0.6546, step time: 0.2706\n",
      "12/517, train_loss: 0.6567, step time: 0.2963\n",
      "13/517, train_loss: 0.6557, step time: 0.2743\n",
      "14/517, train_loss: 0.6345, step time: 0.2701\n",
      "15/517, train_loss: 0.6645, step time: 0.2850\n",
      "16/517, train_loss: 0.6417, step time: 0.2784\n",
      "17/517, train_loss: 0.6363, step time: 0.2753\n",
      "18/517, train_loss: 0.6586, step time: 0.2771\n",
      "19/517, train_loss: 0.6333, step time: 0.2781\n",
      "20/517, train_loss: 0.6500, step time: 0.2716\n",
      "21/517, train_loss: 0.6705, step time: 0.2926\n",
      "22/517, train_loss: 0.6372, step time: 0.2739\n",
      "23/517, train_loss: 0.6440, step time: 0.2711\n",
      "24/517, train_loss: 0.6291, step time: 0.2743\n",
      "25/517, train_loss: 0.6583, step time: 0.3041\n",
      "26/517, train_loss: 0.6243, step time: 0.2767\n",
      "27/517, train_loss: 0.6349, step time: 0.2826\n",
      "28/517, train_loss: 0.6580, step time: 0.2705\n",
      "29/517, train_loss: 0.6356, step time: 0.2682\n",
      "30/517, train_loss: 0.6416, step time: 0.2727\n",
      "31/517, train_loss: 0.6144, step time: 0.2856\n",
      "32/517, train_loss: 0.6180, step time: 0.2735\n",
      "33/517, train_loss: 0.6334, step time: 0.2706\n",
      "34/517, train_loss: 0.6300, step time: 0.2738\n",
      "35/517, train_loss: 0.6438, step time: 0.2840\n",
      "36/517, train_loss: 0.6189, step time: 0.2696\n",
      "37/517, train_loss: 0.6559, step time: 0.2791\n",
      "38/517, train_loss: 0.6227, step time: 0.2771\n",
      "39/517, train_loss: 0.6356, step time: 0.2845\n",
      "40/517, train_loss: 0.6204, step time: 0.2758\n",
      "41/517, train_loss: 0.6394, step time: 0.2903\n",
      "42/517, train_loss: 0.6398, step time: 0.2690\n",
      "43/517, train_loss: 0.6362, step time: 0.2769\n",
      "44/517, train_loss: 0.6257, step time: 0.2725\n",
      "45/517, train_loss: 0.6126, step time: 0.2701\n",
      "46/517, train_loss: 0.6184, step time: 0.2833\n",
      "47/517, train_loss: 0.6359, step time: 0.2790\n",
      "48/517, train_loss: 0.6242, step time: 0.2831\n",
      "49/517, train_loss: 0.6005, step time: 0.2752\n",
      "50/517, train_loss: 0.6193, step time: 0.2776\n",
      "51/517, train_loss: 0.6008, step time: 0.2955\n",
      "52/517, train_loss: 0.6494, step time: 0.2716\n",
      "53/517, train_loss: 0.6177, step time: 0.2769\n",
      "54/517, train_loss: 0.6030, step time: 0.2751\n",
      "55/517, train_loss: 0.6125, step time: 0.3022\n",
      "56/517, train_loss: 0.6396, step time: 0.2682\n",
      "57/517, train_loss: 0.5947, step time: 0.2748\n",
      "58/517, train_loss: 0.6643, step time: 0.2842\n",
      "59/517, train_loss: 0.6434, step time: 0.2699\n",
      "60/517, train_loss: 0.6380, step time: 0.2761\n",
      "61/517, train_loss: 0.6141, step time: 0.2844\n",
      "62/517, train_loss: 0.6437, step time: 0.2904\n",
      "63/517, train_loss: 0.6542, step time: 0.2673\n",
      "64/517, train_loss: 0.5766, step time: 0.2689\n",
      "65/517, train_loss: 0.6072, step time: 0.2683\n",
      "66/517, train_loss: 0.5948, step time: 0.2829\n",
      "67/517, train_loss: 0.6111, step time: 0.2749\n",
      "68/517, train_loss: 0.6511, step time: 0.2969\n",
      "69/517, train_loss: 0.6476, step time: 0.2791\n",
      "70/517, train_loss: 0.6502, step time: 0.2755\n",
      "71/517, train_loss: 0.6098, step time: 0.2782\n",
      "72/517, train_loss: 0.6285, step time: 0.2779\n",
      "73/517, train_loss: 0.6202, step time: 0.2732\n",
      "74/517, train_loss: 0.6105, step time: 0.2800\n",
      "75/517, train_loss: 0.6142, step time: 0.2773\n",
      "76/517, train_loss: 0.6009, step time: 0.2835\n",
      "77/517, train_loss: 0.5789, step time: 0.2767\n",
      "78/517, train_loss: 0.5925, step time: 0.2715\n",
      "79/517, train_loss: 0.6331, step time: 0.2712\n",
      "80/517, train_loss: 0.5770, step time: 0.2746\n",
      "81/517, train_loss: 0.5911, step time: 0.2887\n",
      "82/517, train_loss: 0.6367, step time: 0.2751\n",
      "83/517, train_loss: 0.6182, step time: 0.2847\n",
      "84/517, train_loss: 0.5951, step time: 0.2675\n",
      "85/517, train_loss: 0.6213, step time: 0.2757\n",
      "86/517, train_loss: 0.6094, step time: 0.2725\n",
      "87/517, train_loss: 0.6152, step time: 0.2699\n",
      "88/517, train_loss: 0.6187, step time: 0.2741\n",
      "89/517, train_loss: 0.6052, step time: 0.2836\n",
      "90/517, train_loss: 0.6136, step time: 0.2750\n",
      "91/517, train_loss: 0.6310, step time: 0.2955\n",
      "92/517, train_loss: 0.6112, step time: 0.2683\n",
      "93/517, train_loss: 0.6287, step time: 0.2711\n",
      "94/517, train_loss: 0.5839, step time: 0.2740\n",
      "95/517, train_loss: 0.5646, step time: 0.2742\n",
      "96/517, train_loss: 0.6292, step time: 0.2717\n",
      "97/517, train_loss: 0.6203, step time: 0.2730\n",
      "98/517, train_loss: 0.6157, step time: 0.2723\n",
      "99/517, train_loss: 0.6005, step time: 0.3006\n",
      "100/517, train_loss: 0.6395, step time: 0.2760\n",
      "101/517, train_loss: 0.5979, step time: 0.2762\n",
      "102/517, train_loss: 0.5893, step time: 0.2743\n",
      "103/517, train_loss: 0.6029, step time: 0.2760\n",
      "104/517, train_loss: 0.6046, step time: 0.2705\n",
      "105/517, train_loss: 0.6336, step time: 0.2781\n",
      "106/517, train_loss: 0.6251, step time: 0.2738\n",
      "107/517, train_loss: 0.5891, step time: 0.2853\n",
      "108/517, train_loss: 0.6435, step time: 0.2915\n",
      "109/517, train_loss: 0.5819, step time: 0.2820\n",
      "110/517, train_loss: 0.6083, step time: 0.2701\n",
      "111/517, train_loss: 0.5784, step time: 0.2710\n",
      "112/517, train_loss: 0.5726, step time: 0.2723\n",
      "113/517, train_loss: 0.5656, step time: 0.2730\n",
      "114/517, train_loss: 0.6116, step time: 0.2742\n",
      "115/517, train_loss: 0.6294, step time: 0.2774\n",
      "116/517, train_loss: 0.6695, step time: 0.2935\n",
      "117/517, train_loss: 0.5423, step time: 0.2679\n",
      "118/517, train_loss: 0.6348, step time: 0.2730\n",
      "119/517, train_loss: 0.6090, step time: 0.2839\n",
      "120/517, train_loss: 0.5821, step time: 0.2716\n",
      "121/517, train_loss: 0.6071, step time: 0.2717\n",
      "122/517, train_loss: 0.6357, step time: 0.2782\n",
      "123/517, train_loss: 0.6290, step time: 0.2835\n",
      "124/517, train_loss: 0.5948, step time: 0.2932\n",
      "125/517, train_loss: 0.5847, step time: 0.2690\n",
      "126/517, train_loss: 0.6195, step time: 0.2696\n",
      "127/517, train_loss: 0.6092, step time: 0.2760\n",
      "128/517, train_loss: 0.6116, step time: 0.2880\n",
      "129/517, train_loss: 0.6062, step time: 0.2729\n",
      "130/517, train_loss: 0.6199, step time: 0.2721\n",
      "131/517, train_loss: 0.6094, step time: 0.2739\n",
      "132/517, train_loss: 0.5586, step time: 0.2728\n",
      "133/517, train_loss: 0.6124, step time: 0.2813\n",
      "134/517, train_loss: 0.6193, step time: 0.2714\n",
      "135/517, train_loss: 0.5734, step time: 0.2730\n",
      "136/517, train_loss: 0.6082, step time: 0.2778\n",
      "137/517, train_loss: 0.5884, step time: 0.2759\n",
      "138/517, train_loss: 0.6076, step time: 0.2815\n",
      "139/517, train_loss: 0.6041, step time: 0.2866\n",
      "140/517, train_loss: 0.6075, step time: 0.2708\n",
      "141/517, train_loss: 0.5924, step time: 0.2775\n",
      "142/517, train_loss: 0.6283, step time: 0.2838\n",
      "143/517, train_loss: 0.5672, step time: 0.2975\n",
      "144/517, train_loss: 0.6236, step time: 0.2887\n",
      "145/517, train_loss: 0.5955, step time: 0.2718\n",
      "146/517, train_loss: 0.5940, step time: 0.2756\n",
      "147/517, train_loss: 0.5553, step time: 0.2868\n",
      "148/517, train_loss: 0.5898, step time: 0.2713\n",
      "149/517, train_loss: 0.6217, step time: 0.2739\n",
      "150/517, train_loss: 0.5659, step time: 0.2775\n",
      "151/517, train_loss: 0.5869, step time: 0.2803\n",
      "152/517, train_loss: 0.6200, step time: 0.2893\n",
      "153/517, train_loss: 0.6201, step time: 0.2710\n",
      "154/517, train_loss: 0.6138, step time: 0.2749\n",
      "155/517, train_loss: 0.6216, step time: 0.2886\n",
      "156/517, train_loss: 0.5774, step time: 0.2807\n",
      "157/517, train_loss: 0.5731, step time: 0.2722\n",
      "158/517, train_loss: 0.6146, step time: 0.2747\n",
      "159/517, train_loss: 0.6167, step time: 0.2729\n",
      "160/517, train_loss: 0.6612, step time: 0.2924\n",
      "161/517, train_loss: 0.6206, step time: 0.2682\n",
      "162/517, train_loss: 0.5983, step time: 0.2715\n",
      "163/517, train_loss: 0.5973, step time: 0.2756\n",
      "164/517, train_loss: 0.6234, step time: 0.2868\n",
      "165/517, train_loss: 0.6034, step time: 0.2691\n",
      "166/517, train_loss: 0.5988, step time: 0.2700\n",
      "167/517, train_loss: 0.6094, step time: 0.2759\n",
      "168/517, train_loss: 0.5936, step time: 0.2701\n",
      "169/517, train_loss: 0.5612, step time: 0.2742\n",
      "170/517, train_loss: 0.6219, step time: 0.2749\n",
      "171/517, train_loss: 0.5685, step time: 0.2752\n",
      "172/517, train_loss: 0.6136, step time: 0.2913\n",
      "173/517, train_loss: 0.5464, step time: 0.2680\n",
      "174/517, train_loss: 0.5848, step time: 0.2713\n",
      "175/517, train_loss: 0.6340, step time: 0.2746\n",
      "176/517, train_loss: 0.6270, step time: 0.2881\n",
      "177/517, train_loss: 0.5730, step time: 0.2772\n",
      "178/517, train_loss: 0.6072, step time: 0.2755\n",
      "179/517, train_loss: 0.6148, step time: 0.2711\n",
      "180/517, train_loss: 0.5742, step time: 0.2725\n",
      "181/517, train_loss: 0.6146, step time: 0.2774\n",
      "182/517, train_loss: 0.5850, step time: 0.2802\n",
      "183/517, train_loss: 0.5577, step time: 0.2815\n",
      "184/517, train_loss: 0.5534, step time: 0.2784\n",
      "185/517, train_loss: 0.5976, step time: 0.2886\n",
      "186/517, train_loss: 0.6086, step time: 0.2965\n",
      "187/517, train_loss: 0.6545, step time: 0.2873\n",
      "188/517, train_loss: 0.6359, step time: 0.2706\n",
      "189/517, train_loss: 0.5928, step time: 0.2727\n",
      "190/517, train_loss: 0.5717, step time: 0.2786\n",
      "191/517, train_loss: 0.6093, step time: 0.3032\n",
      "192/517, train_loss: 0.5586, step time: 0.2703\n",
      "193/517, train_loss: 0.5935, step time: 0.2711\n",
      "194/517, train_loss: 0.5495, step time: 0.2840\n",
      "195/517, train_loss: 0.6466, step time: 0.2835\n",
      "196/517, train_loss: 0.5357, step time: 0.2783\n",
      "197/517, train_loss: 0.6452, step time: 0.2775\n",
      "198/517, train_loss: 0.5919, step time: 0.2788\n",
      "199/517, train_loss: 0.5970, step time: 0.2760\n",
      "200/517, train_loss: 0.6228, step time: 0.2734\n",
      "201/517, train_loss: 0.5954, step time: 0.2742\n",
      "202/517, train_loss: 0.5960, step time: 0.2815\n",
      "203/517, train_loss: 0.6043, step time: 0.3052\n",
      "204/517, train_loss: 0.6085, step time: 0.2783\n",
      "205/517, train_loss: 0.5942, step time: 0.2712\n",
      "206/517, train_loss: 0.6459, step time: 0.2717\n",
      "207/517, train_loss: 0.6423, step time: 0.2925\n",
      "208/517, train_loss: 0.5951, step time: 0.2680\n",
      "209/517, train_loss: 0.5989, step time: 0.2710\n",
      "210/517, train_loss: 0.5489, step time: 0.2700\n",
      "211/517, train_loss: 0.5396, step time: 0.2869\n",
      "212/517, train_loss: 0.6171, step time: 0.2698\n",
      "213/517, train_loss: 0.5935, step time: 0.2710\n",
      "214/517, train_loss: 0.6021, step time: 0.2833\n",
      "215/517, train_loss: 0.5863, step time: 0.2841\n",
      "216/517, train_loss: 0.6264, step time: 0.2746\n",
      "217/517, train_loss: 0.6336, step time: 0.2931\n",
      "218/517, train_loss: 0.6301, step time: 0.2718\n",
      "219/517, train_loss: 0.5753, step time: 0.2727\n",
      "220/517, train_loss: 0.5625, step time: 0.2858\n",
      "221/517, train_loss: 0.5325, step time: 0.2768\n",
      "222/517, train_loss: 0.5678, step time: 0.2763\n",
      "223/517, train_loss: 0.6048, step time: 0.2882\n",
      "224/517, train_loss: 0.6308, step time: 0.2772\n",
      "225/517, train_loss: 0.6023, step time: 0.2997\n",
      "226/517, train_loss: 0.5784, step time: 0.2751\n",
      "227/517, train_loss: 0.6076, step time: 0.2814\n",
      "228/517, train_loss: 0.6339, step time: 0.2796\n",
      "229/517, train_loss: 0.5726, step time: 0.2797\n",
      "230/517, train_loss: 0.5803, step time: 0.2920\n",
      "231/517, train_loss: 0.6571, step time: 0.2893\n",
      "232/517, train_loss: 0.6207, step time: 0.2805\n",
      "233/517, train_loss: 0.6102, step time: 0.2737\n",
      "234/517, train_loss: 0.5823, step time: 0.2792\n",
      "235/517, train_loss: 0.5900, step time: 0.2984\n",
      "236/517, train_loss: 0.5787, step time: 0.2717\n",
      "237/517, train_loss: 0.5660, step time: 0.2744\n",
      "238/517, train_loss: 0.5902, step time: 0.2793\n",
      "239/517, train_loss: 0.5802, step time: 0.2961\n",
      "240/517, train_loss: 0.5519, step time: 0.2759\n",
      "241/517, train_loss: 0.5652, step time: 0.2796\n",
      "242/517, train_loss: 0.5896, step time: 0.2790\n",
      "243/517, train_loss: 0.5685, step time: 0.2756\n",
      "244/517, train_loss: 0.6236, step time: 0.2755\n",
      "245/517, train_loss: 0.5990, step time: 0.2749\n",
      "246/517, train_loss: 0.5833, step time: 0.2757\n",
      "247/517, train_loss: 0.5866, step time: 0.3001\n",
      "248/517, train_loss: 0.5986, step time: 0.2688\n",
      "249/517, train_loss: 0.6004, step time: 0.2703\n",
      "250/517, train_loss: 0.5591, step time: 0.2719\n",
      "251/517, train_loss: 0.6399, step time: 0.2845\n",
      "252/517, train_loss: 0.5465, step time: 0.2756\n",
      "253/517, train_loss: 0.5564, step time: 0.2745\n",
      "254/517, train_loss: 0.6149, step time: 0.2796\n",
      "255/517, train_loss: 0.6052, step time: 0.2809\n",
      "256/517, train_loss: 0.5824, step time: 0.2766\n",
      "257/517, train_loss: 0.5765, step time: 0.2796\n",
      "258/517, train_loss: 0.5804, step time: 0.2766\n",
      "259/517, train_loss: 0.5892, step time: 0.2923\n",
      "260/517, train_loss: 0.6004, step time: 0.2695\n",
      "261/517, train_loss: 0.6097, step time: 0.2821\n",
      "262/517, train_loss: 0.6092, step time: 0.2722\n",
      "263/517, train_loss: 0.6250, step time: 0.2958\n",
      "264/517, train_loss: 0.6122, step time: 0.2692\n",
      "265/517, train_loss: 0.5867, step time: 0.2698\n",
      "266/517, train_loss: 0.5793, step time: 0.2719\n",
      "267/517, train_loss: 0.5725, step time: 0.2972\n",
      "268/517, train_loss: 0.5495, step time: 0.2739\n",
      "269/517, train_loss: 0.5990, step time: 0.2851\n",
      "270/517, train_loss: 0.6124, step time: 0.2740\n",
      "271/517, train_loss: 0.5522, step time: 0.2840\n",
      "272/517, train_loss: 0.5670, step time: 0.2787\n",
      "273/517, train_loss: 0.5785, step time: 0.2848\n",
      "274/517, train_loss: 0.5935, step time: 0.2710\n",
      "275/517, train_loss: 0.5430, step time: 0.2748\n",
      "276/517, train_loss: 0.5892, step time: 0.2895\n",
      "277/517, train_loss: 0.6592, step time: 0.2938\n",
      "278/517, train_loss: 0.5882, step time: 0.2757\n",
      "279/517, train_loss: 0.5578, step time: 0.2747\n",
      "280/517, train_loss: 0.5794, step time: 0.2888\n",
      "281/517, train_loss: 0.5781, step time: 0.2930\n",
      "282/517, train_loss: 0.5884, step time: 0.2763\n",
      "283/517, train_loss: 0.5817, step time: 0.2769\n",
      "284/517, train_loss: 0.5456, step time: 0.2742\n",
      "285/517, train_loss: 0.5247, step time: 0.2754\n",
      "286/517, train_loss: 0.6029, step time: 0.2850\n",
      "287/517, train_loss: 0.5900, step time: 0.2790\n",
      "288/517, train_loss: 0.6293, step time: 0.2790\n",
      "289/517, train_loss: 0.6078, step time: 0.2796\n",
      "290/517, train_loss: 0.5614, step time: 0.2902\n",
      "291/517, train_loss: 0.5314, step time: 0.2853\n",
      "292/517, train_loss: 0.5435, step time: 0.2810\n",
      "293/517, train_loss: 0.5947, step time: 0.2958\n",
      "294/517, train_loss: 0.5740, step time: 0.2763\n",
      "295/517, train_loss: 0.5632, step time: 0.2858\n",
      "296/517, train_loss: 0.6031, step time: 0.2768\n",
      "297/517, train_loss: 0.5962, step time: 0.2924\n",
      "298/517, train_loss: 0.6116, step time: 0.2947\n",
      "299/517, train_loss: 0.5397, step time: 0.3026\n",
      "300/517, train_loss: 0.5984, step time: 0.2869\n",
      "301/517, train_loss: 0.6119, step time: 0.2750\n",
      "302/517, train_loss: 0.5669, step time: 0.2727\n",
      "303/517, train_loss: 0.6320, step time: 0.2806\n",
      "304/517, train_loss: 0.5974, step time: 0.2733\n",
      "305/517, train_loss: 0.5872, step time: 0.2829\n",
      "306/517, train_loss: 0.5652, step time: 0.2925\n",
      "307/517, train_loss: 0.5575, step time: 0.2914\n",
      "308/517, train_loss: 0.5295, step time: 0.2971\n",
      "309/517, train_loss: 0.5714, step time: 0.2701\n",
      "310/517, train_loss: 0.5813, step time: 0.2751\n",
      "311/517, train_loss: 0.5404, step time: 0.2862\n",
      "312/517, train_loss: 0.6149, step time: 0.2712\n",
      "313/517, train_loss: 0.5862, step time: 0.2708\n",
      "314/517, train_loss: 0.5835, step time: 0.2740\n",
      "315/517, train_loss: 0.5659, step time: 0.2779\n",
      "316/517, train_loss: 0.5744, step time: 0.2969\n",
      "317/517, train_loss: 0.6312, step time: 0.2866\n",
      "318/517, train_loss: 0.5833, step time: 0.2792\n",
      "319/517, train_loss: 0.5964, step time: 0.2805\n",
      "320/517, train_loss: 0.5927, step time: 0.2782\n",
      "321/517, train_loss: 0.6182, step time: 0.2927\n",
      "322/517, train_loss: 0.6195, step time: 0.2789\n",
      "323/517, train_loss: 0.6297, step time: 0.2794\n",
      "324/517, train_loss: 0.5209, step time: 0.2768\n",
      "325/517, train_loss: 0.5571, step time: 0.2749\n",
      "326/517, train_loss: 0.5894, step time: 0.2821\n",
      "327/517, train_loss: 0.5513, step time: 0.2801\n",
      "328/517, train_loss: 0.5653, step time: 0.2743\n",
      "329/517, train_loss: 0.6124, step time: 0.2816\n",
      "330/517, train_loss: 0.5558, step time: 0.2711\n",
      "331/517, train_loss: 0.5562, step time: 0.2777\n",
      "332/517, train_loss: 0.6026, step time: 0.2780\n",
      "333/517, train_loss: 0.5742, step time: 0.2809\n",
      "334/517, train_loss: 0.5767, step time: 0.2750\n",
      "335/517, train_loss: 0.5686, step time: 0.2764\n",
      "336/517, train_loss: 0.5333, step time: 0.2881\n",
      "337/517, train_loss: 0.5604, step time: 0.2745\n",
      "338/517, train_loss: 0.5272, step time: 0.2763\n",
      "339/517, train_loss: 0.6088, step time: 0.2977\n",
      "340/517, train_loss: 0.5533, step time: 0.2753\n",
      "341/517, train_loss: 0.5811, step time: 0.2818\n",
      "342/517, train_loss: 0.5667, step time: 0.3043\n",
      "343/517, train_loss: 0.5723, step time: 0.2987\n",
      "344/517, train_loss: 0.5990, step time: 0.2725\n",
      "345/517, train_loss: 0.5469, step time: 0.2834\n",
      "346/517, train_loss: 0.5545, step time: 0.2733\n",
      "347/517, train_loss: 0.5557, step time: 0.2959\n",
      "348/517, train_loss: 0.5665, step time: 0.2692\n",
      "349/517, train_loss: 0.5880, step time: 0.2743\n",
      "350/517, train_loss: 0.5690, step time: 0.2756\n",
      "351/517, train_loss: 0.5724, step time: 0.2734\n",
      "352/517, train_loss: 0.5981, step time: 0.2987\n",
      "353/517, train_loss: 0.5652, step time: 0.2842\n",
      "354/517, train_loss: 0.5417, step time: 0.2723\n",
      "355/517, train_loss: 0.5920, step time: 0.2704\n",
      "356/517, train_loss: 0.5651, step time: 0.2717\n",
      "357/517, train_loss: 0.5551, step time: 0.2753\n",
      "358/517, train_loss: 0.5917, step time: 0.2778\n",
      "359/517, train_loss: 0.6031, step time: 0.2859\n",
      "360/517, train_loss: 0.5638, step time: 0.2720\n",
      "361/517, train_loss: 0.5928, step time: 0.2821\n",
      "362/517, train_loss: 0.5874, step time: 0.2703\n",
      "363/517, train_loss: 0.6035, step time: 0.2964\n",
      "364/517, train_loss: 0.5751, step time: 0.2749\n",
      "365/517, train_loss: 0.6365, step time: 0.2833\n",
      "366/517, train_loss: 0.5638, step time: 0.2716\n",
      "367/517, train_loss: 0.5647, step time: 0.2923\n",
      "368/517, train_loss: 0.5226, step time: 0.2714\n",
      "369/517, train_loss: 0.5871, step time: 0.2721\n",
      "370/517, train_loss: 0.5813, step time: 0.2821\n",
      "371/517, train_loss: 0.5907, step time: 0.2911\n",
      "372/517, train_loss: 0.5871, step time: 0.2768\n",
      "373/517, train_loss: 0.5985, step time: 0.2785\n",
      "374/517, train_loss: 0.5942, step time: 0.2773\n",
      "375/517, train_loss: 0.6007, step time: 0.2872\n",
      "376/517, train_loss: 0.5812, step time: 0.2708\n",
      "377/517, train_loss: 0.5438, step time: 0.2732\n",
      "378/517, train_loss: 0.5086, step time: 0.2756\n",
      "379/517, train_loss: 0.5801, step time: 0.2730\n",
      "380/517, train_loss: 0.5825, step time: 0.2741\n",
      "381/517, train_loss: 0.5563, step time: 0.2813\n",
      "382/517, train_loss: 0.6097, step time: 0.2831\n",
      "383/517, train_loss: 0.5204, step time: 0.2757\n",
      "384/517, train_loss: 0.6128, step time: 0.2957\n",
      "385/517, train_loss: 0.5756, step time: 0.2692\n",
      "386/517, train_loss: 0.5263, step time: 0.2755\n",
      "387/517, train_loss: 0.6531, step time: 0.3013\n",
      "388/517, train_loss: 0.5698, step time: 0.2708\n",
      "389/517, train_loss: 0.5687, step time: 0.2726\n",
      "390/517, train_loss: 0.5708, step time: 0.2753\n",
      "391/517, train_loss: 0.6097, step time: 0.2905\n",
      "392/517, train_loss: 0.5518, step time: 0.2694\n",
      "393/517, train_loss: 0.5434, step time: 0.2697\n",
      "394/517, train_loss: 0.5444, step time: 0.2725\n",
      "395/517, train_loss: 0.5616, step time: 0.2733\n",
      "396/517, train_loss: 0.5810, step time: 0.2767\n",
      "397/517, train_loss: 0.5604, step time: 0.2786\n",
      "398/517, train_loss: 0.5929, step time: 0.2805\n",
      "399/517, train_loss: 0.5843, step time: 0.2842\n",
      "400/517, train_loss: 0.5453, step time: 0.2722\n",
      "401/517, train_loss: 0.5425, step time: 0.2827\n",
      "402/517, train_loss: 0.5192, step time: 0.2731\n",
      "403/517, train_loss: 0.5614, step time: 0.2875\n",
      "404/517, train_loss: 0.6016, step time: 0.2702\n",
      "405/517, train_loss: 0.5636, step time: 0.2731\n",
      "406/517, train_loss: 0.5601, step time: 0.2726\n",
      "407/517, train_loss: 0.5524, step time: 0.2733\n",
      "408/517, train_loss: 0.5507, step time: 0.2766\n",
      "409/517, train_loss: 0.5726, step time: 0.2909\n",
      "410/517, train_loss: 0.5528, step time: 0.2819\n",
      "411/517, train_loss: 0.5800, step time: 0.2726\n",
      "412/517, train_loss: 0.5835, step time: 0.2773\n",
      "413/517, train_loss: 0.6191, step time: 0.3077\n",
      "414/517, train_loss: 0.5920, step time: 0.2757\n",
      "415/517, train_loss: 0.6037, step time: 0.2724\n",
      "416/517, train_loss: 0.5101, step time: 0.2785\n",
      "417/517, train_loss: 0.5714, step time: 0.2938\n",
      "418/517, train_loss: 0.6074, step time: 0.2686\n",
      "419/517, train_loss: 0.5127, step time: 0.2724\n",
      "420/517, train_loss: 0.4952, step time: 0.2739\n",
      "421/517, train_loss: 0.5886, step time: 0.2797\n",
      "422/517, train_loss: 0.6395, step time: 0.2765\n",
      "423/517, train_loss: 0.5505, step time: 0.2754\n",
      "424/517, train_loss: 0.5630, step time: 0.2851\n",
      "425/517, train_loss: 0.5996, step time: 0.2939\n",
      "426/517, train_loss: 0.5802, step time: 0.2725\n",
      "427/517, train_loss: 0.5458, step time: 0.2760\n",
      "428/517, train_loss: 0.5747, step time: 0.2823\n",
      "429/517, train_loss: 0.5552, step time: 0.2920\n",
      "430/517, train_loss: 0.5930, step time: 0.2886\n",
      "431/517, train_loss: 0.5339, step time: 0.2718\n",
      "432/517, train_loss: 0.4821, step time: 0.2713\n",
      "433/517, train_loss: 0.6591, step time: 0.2862\n",
      "434/517, train_loss: 0.5517, step time: 0.2716\n",
      "435/517, train_loss: 0.6009, step time: 0.2703\n",
      "436/517, train_loss: 0.5935, step time: 0.2741\n",
      "437/517, train_loss: 0.5474, step time: 0.2740\n",
      "438/517, train_loss: 0.5366, step time: 0.2887\n",
      "439/517, train_loss: 0.5670, step time: 0.2865\n",
      "440/517, train_loss: 0.5149, step time: 0.2882\n",
      "441/517, train_loss: 0.5517, step time: 0.2794\n",
      "442/517, train_loss: 0.6113, step time: 0.2898\n",
      "443/517, train_loss: 0.5705, step time: 0.2721\n",
      "444/517, train_loss: 0.6001, step time: 0.2763\n",
      "445/517, train_loss: 0.5847, step time: 0.2908\n",
      "446/517, train_loss: 0.5621, step time: 0.2795\n",
      "447/517, train_loss: 0.5438, step time: 0.2856\n",
      "448/517, train_loss: 0.5876, step time: 0.2736\n",
      "449/517, train_loss: 0.5205, step time: 0.2871\n",
      "450/517, train_loss: 0.5306, step time: 0.2791\n",
      "451/517, train_loss: 0.5119, step time: 0.2999\n",
      "452/517, train_loss: 0.5575, step time: 0.3020\n",
      "453/517, train_loss: 0.5347, step time: 0.3048\n",
      "454/517, train_loss: 0.5253, step time: 0.2893\n",
      "455/517, train_loss: 0.5542, step time: 0.2948\n",
      "456/517, train_loss: 0.5425, step time: 0.3002\n",
      "457/517, train_loss: 0.5248, step time: 0.3032\n",
      "458/517, train_loss: 0.5152, step time: 0.2913\n",
      "459/517, train_loss: 0.5669, step time: 0.2979\n",
      "460/517, train_loss: 0.5916, step time: 0.2720\n",
      "461/517, train_loss: 0.5900, step time: 0.2739\n",
      "462/517, train_loss: 0.5191, step time: 0.2846\n",
      "463/517, train_loss: 0.5269, step time: 0.2778\n",
      "464/517, train_loss: 0.6087, step time: 0.2809\n",
      "465/517, train_loss: 0.6304, step time: 0.2992\n",
      "466/517, train_loss: 0.6033, step time: 0.2732\n",
      "467/517, train_loss: 0.5404, step time: 0.2755\n",
      "468/517, train_loss: 0.5402, step time: 0.2733\n",
      "469/517, train_loss: 0.5967, step time: 0.2923\n",
      "470/517, train_loss: 0.5385, step time: 0.2726\n",
      "471/517, train_loss: 0.5479, step time: 0.2723\n",
      "472/517, train_loss: 0.5945, step time: 0.2729\n",
      "473/517, train_loss: 0.5757, step time: 0.3078\n",
      "474/517, train_loss: 0.5298, step time: 0.2679\n",
      "475/517, train_loss: 0.5820, step time: 0.2699\n",
      "476/517, train_loss: 0.4984, step time: 0.2713\n",
      "477/517, train_loss: 0.5445, step time: 0.2768\n",
      "478/517, train_loss: 0.6109, step time: 0.2723\n",
      "479/517, train_loss: 0.5747, step time: 0.2741\n",
      "480/517, train_loss: 0.5334, step time: 0.2783\n",
      "481/517, train_loss: 0.5933, step time: 0.2984\n",
      "482/517, train_loss: 0.5457, step time: 0.2745\n",
      "483/517, train_loss: 0.5812, step time: 0.2804\n",
      "484/517, train_loss: 0.4823, step time: 0.2828\n",
      "485/517, train_loss: 0.5316, step time: 0.2771\n",
      "486/517, train_loss: 0.5282, step time: 0.2886\n",
      "487/517, train_loss: 0.5341, step time: 0.2885\n",
      "488/517, train_loss: 0.5988, step time: 0.2785\n",
      "489/517, train_loss: 0.5963, step time: 0.2868\n",
      "490/517, train_loss: 0.5153, step time: 0.2712\n",
      "491/517, train_loss: 0.5862, step time: 0.2775\n",
      "492/517, train_loss: 0.4876, step time: 0.2869\n",
      "493/517, train_loss: 0.5264, step time: 0.2953\n",
      "494/517, train_loss: 0.5401, step time: 0.2786\n",
      "495/517, train_loss: 0.5605, step time: 0.2842\n",
      "496/517, train_loss: 0.5800, step time: 0.2834\n",
      "497/517, train_loss: 0.5744, step time: 0.2791\n",
      "498/517, train_loss: 0.5423, step time: 0.2937\n",
      "499/517, train_loss: 0.5921, step time: 0.2911\n",
      "500/517, train_loss: 0.6335, step time: 0.2800\n",
      "501/517, train_loss: 0.5389, step time: 0.2787\n",
      "502/517, train_loss: 0.5428, step time: 0.2857\n",
      "503/517, train_loss: 0.5398, step time: 0.2950\n",
      "504/517, train_loss: 0.5626, step time: 0.2946\n",
      "505/517, train_loss: 0.5766, step time: 0.2735\n",
      "506/517, train_loss: 0.6045, step time: 0.2733\n",
      "507/517, train_loss: 0.5656, step time: 0.2813\n",
      "508/517, train_loss: 0.5030, step time: 0.2809\n",
      "509/517, train_loss: 0.5091, step time: 0.2838\n",
      "510/517, train_loss: 0.5255, step time: 0.2998\n",
      "511/517, train_loss: 0.5045, step time: 0.3046\n",
      "512/517, train_loss: 0.4954, step time: 0.2785\n",
      "513/517, train_loss: 0.5830, step time: 0.2870\n",
      "514/517, train_loss: 0.5841, step time: 0.2707\n",
      "515/517, train_loss: 0.5879, step time: 0.2770\n",
      "516/517, train_loss: 0.5537, step time: 0.2906\n",
      "517/517, train_loss: 0.5499, step time: 0.2694\n",
      "518/517, train_loss: 0.5608, step time: 1.2111\n",
      "epoch 1 average loss: 0.5901\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'inference' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 53\u001b[0m\n\u001b[1;32m     48\u001b[0m label \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([val_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mround()\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m,val_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mround()\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m,val_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mround()\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m2\u001b[39m],dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     49\u001b[0m val_inputs, val_labels \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     50\u001b[0m     val_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m     51\u001b[0m     label,\n\u001b[1;32m     52\u001b[0m )\n\u001b[0;32m---> 53\u001b[0m val_outputs \u001b[38;5;241m=\u001b[39m \u001b[43minference\u001b[49m(val_inputs)\n\u001b[1;32m     54\u001b[0m val_outputs \u001b[38;5;241m=\u001b[39m [post_trans(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m decollate_batch(val_outputs)]\n\u001b[1;32m     55\u001b[0m dice_metric(y_pred\u001b[38;5;241m=\u001b[39mval_outputs, y\u001b[38;5;241m=\u001b[39mval_labels)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'inference' is not defined"
     ]
    }
   ],
   "source": [
    "est_metric = -1\n",
    "best_metric_epoch = -1\n",
    "best_metrics_epochs_and_time = [[], [], []]\n",
    "epoch_loss_values = []\n",
    "metric_values = []\n",
    "metric_values_tc = []\n",
    "metric_values_wt = []\n",
    "metric_values_et = []\n",
    "\n",
    "total_start = time.time()\n",
    "for epoch in range(max_epochs):\n",
    "    epoch_start = time.time()\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    for batch_data in train_loader:\n",
    "        step_start = time.time()\n",
    "        step += 1\n",
    "        label = torch.cat([batch_data[\"label\"].round()==0,batch_data[\"label\"].round()==1,batch_data[\"label\"].round()==2],dim=1)\n",
    "        inputs, labels = (\n",
    "            batch_data[\"input\"].to(device),\n",
    "            label.to(device),\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, labels)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        epoch_loss += loss.item()\n",
    "        print(\n",
    "            f\"{step}/{len(train_ds) // train_loader.batch_size}\"\n",
    "            f\", train_loss: {loss.item():.4f}\"\n",
    "            f\", step time: {(time.time() - step_start):.4f}\"\n",
    "        )\n",
    "    lr_scheduler.step()\n",
    "    epoch_loss /= step\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for val_data in val_loader:\n",
    "                label = torch.cat([val_data[\"label\"].round()==0,val_data[\"label\"].round()==1,val_data[\"label\"].round()==2],dim=1)\n",
    "                val_inputs, val_labels = (\n",
    "                    val_data[\"input\"].to(device),\n",
    "                    label,\n",
    "                )\n",
    "                val_outputs = inference(val_inputs)\n",
    "                val_outputs = [post_trans(i) for i in decollate_batch(val_outputs)]\n",
    "                dice_metric(y_pred=val_outputs, y=val_labels)\n",
    "                dice_metric_batch(y_pred=val_outputs, y=val_labels)\n",
    "\n",
    "            metric = dice_metric.aggregate().item()\n",
    "            metric_values.append(metric)\n",
    "            metric_batch = dice_metric_batch.aggregate()\n",
    "            metric_tc = metric_batch[0].item()\n",
    "            metric_values_tc.append(metric_tc)\n",
    "            metric_wt = metric_batch[1].item()\n",
    "            metric_values_wt.append(metric_wt)\n",
    "            metric_et = metric_batch[2].item()\n",
    "            metric_values_et.append(metric_et)\n",
    "            dice_metric.reset()\n",
    "            dice_metric_batch.reset()\n",
    "\n",
    "            if metric > best_metric:\n",
    "                best_metric = metric\n",
    "                best_metric_epoch = epoch + 1\n",
    "                best_metrics_epochs_and_time[0].append(best_metric)\n",
    "                best_metrics_epochs_and_time[1].append(best_metric_epoch)\n",
    "                best_metrics_epochs_and_time[2].append(time.time() - total_start)\n",
    "                torch.save(\n",
    "                    model.state_dict(),\n",
    "                    os.path.join(root_dir, \"best_metric_model.pth\"),\n",
    "                )\n",
    "                print(\"saved new best metric model\")\n",
    "            print(\n",
    "                f\"current epoch: {epoch + 1} current mean dice: {metric:.4f}\"\n",
    "                f\" tc: {metric_tc:.4f} wt: {metric_wt:.4f} et: {metric_et:.4f}\"\n",
    "                f\"\\nbest mean dice: {best_metric:.4f}\"\n",
    "                f\" at epoch: {best_metric_epoch}\"\n",
    "            )\n",
    "    print(f\"time consuming of epoch {epoch + 1} is: {(time.time() - epoch_start):.4f}\")\n",
    "total_time = time.time() - total_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8fb2fbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, labels = (\n",
    "    batch_data[\"input\"].to(device),\n",
    "    batch_data[\"label\"].to(device),\n",
    ")\n",
    "optimizer.zero_grad()\n",
    "with torch.cuda.amp.autocast():\n",
    "    outputs = model(inputs)\n",
    "    loss = loss_function(outputs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724f005a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"train completed, best_metric: {best_metric:.4f} at epoch: {best_metric_epoch}, total time: {total_time}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
